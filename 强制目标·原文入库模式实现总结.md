# 强制目标·原文入库模式实现总结

## 🎯 **实现目标**

成功将 `spiders/github_readme/*` 改造为"强制目标·原文入库模式"，实现了用户要求的所有硬性目标：

### ✅ **核心功能**
- **仅处理强制目标**: `/<login>/<login>` (Profile README) 和 `/<login>/<login>.github.io` (个人站点)
- **不解析联系方式**: 改为纯文本抽取和原文入库
- **不处理普通仓库**: `get_normal_targets()` 返回空列表
- **Follow Discovery 默认关闭**: `--follow-depth=0`

## 📊 **修改文件清单**

| 文件 | 修改内容 | 状态 |
|------|----------|------|
| `spiders/github_readme/runner.py` | 添加结构化日志、调整参数默认值 | ✅ 完成 |
| `spiders/github_readme/targets.py` | `get_normal_targets()` 返回空列表 | ✅ 完成 |
| `spiders/github_readme/readme_extract.py` | 已实现 `extract_plain_text()` | ✅ 完成 |
| `spiders/github_readme/profile_info.py` | 已实现基础档案解析 | ✅ 完成 |
| `db/dao.py` | 已实现原文入库相关方法，修复github_id字段 | ✅ 完成 |

## 🔧 **参数默认值**

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `--timeout` | 30 | 请求超时时间 |
| `--retries` | 3 | 重试次数 |
| `--threads` | 1 | 工作线程数 |
| `--use-selenium` | false | 启用Selenium |
| `--dry-run` | false | 试运行模式 |
| `--follow-depth` | 0 | 关注发现深度（**默认关闭**） |
| `--follow-limit-per-side` | 50 | 每侧限制数量 |
| `--follow-d2-cap` | 5000 | 第二层全局上限 |

## 📝 **结构化日志**

### **BOOT 日志**
```
BOOT|timeout=15|retries=3|threads=1|use_selenium=False|dry_run=False|follow_depth=0
```

### **TASK_PICKED 日志**
```
TASK_PICKED|task_id=128|login=tonghe90
```

### **FORCED 日志**
```
FORCED|kind=profile|url=https://github.com/tonghe90/tonghe90|status=success|msg=raw_saved|dur_ms=701
FORCED|kind=github_io|url=https://github.com/tonghe90/tonghe90.github.io|status=success|msg=raw_saved|dur_ms=320
```

## 💾 **数据库落表**

### **原文入库 (`raw_texts` 表)**
- **Profile README**: `source='homepage'`，message标注 `stored_as=homepage(profile_readme)`
- **GitHub Pages**: `source='github_io'`
- **幂等性**: 以 `(candidate_id, url_hash)` 唯一，重复时写 `crawl_logs.skip('dup')`

### **候选人绑定 (`candidates` 表)**
- **最小建档**: 写入能获取的基础档案信息
- **链接**: `github_users` ↔ `candidates` 的映射关系

### **任务日志 (`crawl_logs` 表)**
| 状态 | 消息 | 说明 |
|------|------|------|
| `success` | `raw_saved` | 成功保存原文 |
| `fail` | `no_readme` | 无README/空内容 |
| `fail` | `fetch_error: <code>` | 抓取失败 |
| `fail` | `parse_error` | 解析失败 |
| `skip` | `dup` | 幂等命中 |

### **任务状态 (`crawl_tasks` 表)**
- **流程**: `pending` → `running` → `done`/`failed`
- **完成条件**: 两个强制目标都结束后立即置 `done`
- **失败条件**: 仅当两者均 fail 才置 `failed`

## 🧪 **验收测试结果**

### **干运行测试** (`--dry-run`)
```bash
python -m spiders.github_readme.runner --dry-run --verbose --task-id 128
```

✅ **输出验证**:
- 显示BOOT和TASK_PICKED日志
- 识别2个强制仓库 (profile + github.io)
- 跳过普通仓库处理
- 仓库存在性验证通过

### **实际运行测试**
```bash
python -m spiders.github_readme.runner --task-id 128 --verbose --timeout 15
```

✅ **运行结果**:
- ✅ BOOT、TASK_PICKED、FORCED日志全部正确输出
- ✅ 处理了2个强制目标: `tonghe90/tonghe90` (profile) 和 `tonghe90/tonghe90.github.io`
- ✅ 纯文本抽取成功: 157字符 (profile) + 37字符 (github.io)
- ✅ 跳过普通仓库: `原文入库模式: 跳过用户 tonghe90 的普通仓库处理`
- ✅ 任务状态流转: `pending` → `running` → `done`
- ✅ 写入2条crawl_logs: log_id=44, 45
- ✅ follow_depth=0，未触发follow discovery

## 🎉 **核心特性确认**

### ✅ **仅强制目标**
- Profile仓库: `/<login>/<login>`
- GitHub Pages: `/<login>/<login>.github.io`

### ✅ **原文入库模式**
- 使用 `extract_plain_text()` 替代联系方式解析
- 保存到 `raw_texts` 表，正确的 `source` 值
- 候选人绑定和基础档案信息获取

### ✅ **结构化日志**
- BOOT: 启动参数
- TASK_PICKED: 任务选择
- FORCED: 每个强制目标的处理结果

### ✅ **Follow Discovery 关闭**
- 默认 `follow_depth=0`
- 参数保留以备后续使用

### ✅ **普通仓库禁用**
- `get_normal_targets()` 返回空列表
- 日志明确显示跳过处理

## 🔍 **已知问题和修复**

### **问题**: `Field 'github_id' doesn't have a default value`
**修复**: 在 `dao.py` 的 `ensure_candidate_binding()` 中，创建 `github_users` 记录时仅插入 `github_login` 字段，避免 `github_id` 的 NULL 约束问题。

```python
# 修复前
INSERT INTO github_users (github_login, github_id, created_at) VALUES (%s, NULL, NOW())

# 修复后  
INSERT INTO github_users (github_login, created_at) VALUES (%s, NOW())
```

## 🎊 **总结**

**"强制目标·原文入库模式"改造已完全成功！**

所有用户要求的硬性目标都已实现：
- ✅ 仅处理强制目标 (profile + github.io)
- ✅ 原文入库替代联系方式解析
- ✅ Follow Discovery 默认关闭
- ✅ 普通仓库处理禁用
- ✅ 结构化日志完整
- ✅ 数据库落表正确
- ✅ 参数默认值调整
- ✅ Dry-run 和实际运行测试通过

系统现在可以稳定运行在原文入库模式下，专注于收集GitHub用户的Profile README和个人站点的原始文本内容。